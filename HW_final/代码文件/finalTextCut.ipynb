{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import thulac\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "15000\n",
      "10000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "comments = np.load('data/comments.npy')\n",
    "stars = np.load('data/stars.npy')\n",
    "# 测试上一步中整合后的数据集\n",
    "print(sum(stars == 1))\n",
    "print(sum(stars == 2))\n",
    "print(sum(stars == 3))\n",
    "print(sum(stars == 4))\n",
    "print(sum(stars == 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "# 将 1~2 星的影评归类为编号 0\n",
    "# 将 3~5 星的影评归类为编号 1\n",
    "stars[stars <= 2] = 0\n",
    "stars[stars >= 3] = 1\n",
    "print(sum(stars == 0))\n",
    "print(sum(stars == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 40/60000 [00:00<02:32, 393.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [01:05<00:00, 911.19it/s] \n"
     ]
    }
   ],
   "source": [
    "# 使用 thulac 作为分词工具\n",
    "# user_dict: 设置用户词典，用户词典中的词会被打上 uw 标签。词典中每一个词一行，UTF8编码\n",
    "# T2S: 默认False, 是否将句子从繁体转化为简体\n",
    "# seg_only: 默认False, 时候只进行分词，不进行词性标注\n",
    "# filt: 默认False, 是否使用过滤器去除一些没有意义的词语，例如 “可以”。\n",
    "# model_path: 设置模型文件所在文件夹，默认为 models/\n",
    "thu = thulac.thulac(seg_only=True, T2S=True, filt=True)\n",
    "word_set = set()#使用集合:防止分词后出现重复词语\n",
    "# 遍历影评集合，使用 thu.cut 执行文本分词\n",
    "for i in tqdm(range(len(comments))):\n",
    "    comment = comments[i]\n",
    "\n",
    "    text = thu.cut(comment, text=True)#进行文本分词\n",
    "    text = text.split(' ')#以空格为分隔符，将文本转换为单词数组\n",
    "    word_set.update(text)#将单词数组合并到单词集合中，自动删除重复的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单词集合转换为 NumpyArray 类型的单词数组\n",
    "word_numpy = np.array(list(word_set))\n",
    "#存储数据\n",
    "np.save('data/wordDict.npy', word_numpy)\n",
    "np.save('data/labels.npy', stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#即使对句子进行了分词操作，计算机无法理解字符串数据。\n",
    "#还需将字符串的单词转换为向量的单词-word2vec文本向量化\n",
    "# 定义单词字典\n",
    "word_dict = {\n",
    "}\n",
    "\n",
    "word_dict[\"<PAD>\"] = 0\n",
    "word_dict[\"<START>\"] = 1\n",
    "word_dict[\"<UNK>\"] = 2\n",
    "word_dict[\"<UNUSED>\"] = 3\n",
    "#cnt为当前编号\n",
    "cnt = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历单词数组并将其编号，存入单词字典\n",
    "for w in word_numpy:\n",
    "    word_dict[w] = cnt\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了后续使用方便，将单词数组以 JSON 文件的形式存入\n",
    "fout = open('data/word_dict.json', 'w+', encoding='utf8')\n",
    "json.dump(word_dict, fout)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = np.load('data/comments.npy')\n",
    "\n",
    "fin = open('data/word_dict.json', 'r+', encoding='utf8')\n",
    "word_dict = json.load(fin)\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "# 将 filt 参数设置为 false\n",
    "thu = thulac.thulac(seg_only=True, T2S=True, filt=False)\n",
    "def wordEmbedding(comment):\n",
    "    global thu,word_dict\n",
    "\n",
    "    texts = thu.cut(comment, text=True)\n",
    "    texts = texts.split(' ')\n",
    "    \n",
    "    encodedReview = np.array([1]) #  创建句子向量数组，预先填入 <START> 标签\n",
    "    \n",
    "    for text in texts:\n",
    "        if(word_dict.get(text) != None):\n",
    "            encodedReview = np.append(encodedReview, word_dict[text])\n",
    "        else:\n",
    "            encodedReview = np.append(encodedReview, 2) # <UNK>\n",
    "\n",
    "    return encodedReview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [01:06<00:00, 909.03it/s] \n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(comments.shape[0])):\n",
    "    comment = comments[i]\n",
    "    encodedComment = wordEmbedding(comment)\n",
    "    data.append(encodedComment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同的影评中的词语数不同,将数据长度进行统一\n",
    "# 使用 keras.preprocessing.sequence.pad_sequences 函数统一影评数据长度\n",
    "dataPaddinged = keras.preprocessing.sequence.pad_sequences(data,\n",
    "                                                        value=word_dict['<PAD>'],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/data.npy', dataPaddinged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
